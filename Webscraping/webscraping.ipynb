{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taller de web scraping con Python\n",
    "# Diplomado en Ciencia de Datos UC\n",
    "# Sesión a cargo de Riva Quiroga (https://rivaquiroga.cl)\n",
    "\n",
    "# Ejercicio 1\n",
    "\n",
    "# Importacion de datos \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import math\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 0: Guardar la URL del sitio en una variable\n",
    "\n",
    "sitio = \"https://rivaquiroga.github.io/taller-web-scraping-python-2023/ejercicio-1.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Hacer una \"solicitud\" a la página web para traer el código fuente\n",
    "respuesta = requests.get(sitio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2:\n",
    "contenido = respuesta.text\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: crear la sopa\n",
    "\n",
    "soup = BeautifulSoup(contenido, \"html.parser\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4: buscar los datos que nos interesan dentro del código fuente de la página\n",
    "\n",
    "La función find() nos permite encontrar el primer elemento que tenga una determinada etiqueta/clase. nos devuelve todo el elemento html (o sea, las etiqutas y el contenido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"h2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para quedarnos solo con el texto se aplica el método **.get_text()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Librerías para hacer web scraping'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"h1\").get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos encontrar todos los elementos que tengan una determinada etiqueta/clase, usamos el método **.find_all()**. eso nos devuelve una lista con todos esos elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2>BeautifulSoup</h2>, <h2>Selenium</h2>, <h2>Scrapy</h2>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"h2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No podemos utilizar get_text() con find_all() porque ese  método funciona con cada elemento de forma individual, no con una lista:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m soup\u001b[39m.\u001b[39;49mfind_all(\u001b[39m\"\u001b[39;49m\u001b[39mh2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mget_text()\n",
      "File \u001b[1;32mc:\\Users\\anddiazc\\AppData\\Local\\miniconda3\\envs\\default\\lib\\site-packages\\bs4\\element.py:2428\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2426\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   2427\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2428\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m   2429\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mResultSet object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. You\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key\n\u001b[0;32m   2430\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "soup.find_all(\"h2\").get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tenemos que aplicarlo a cada elemento de forma individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BeautifulSoup'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"h2\")[0].get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la opción más rápida sería iterar. los pasos entonces serían:\n",
    "\n",
    "Primero guardar en una variable la lista con todos los elementos h2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos_h2 = soup.find_all(\"h2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, iterar por los elementos de esa lista, aplicarles get_text() y guardar el texto en la lista librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "librerias = []\n",
    "\n",
    "for elemento in elementos_h2:\n",
    "    elemento = elemento.get_text()\n",
    "    librerias.append(elemento)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BeautifulSoup', 'Selenium', 'Scrapy']\n"
     ]
    }
   ],
   "source": [
    "print(librerias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, extraigamos las descripciones:\n",
    "Como hay varias cosas etiqquetados como p, tenemos que especificar la calse para que nos devuevla, solo la que nos interesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos_p = soup.find_all(\"p\", class_ =\"librerias\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siempre es útil chequear con len() si la canitdad de elementos es la que esperábamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "descripciones = []\n",
    "for elemento in elementos_p:\n",
    "    elemento = elemento.get_text(strip = True)\n",
    "    descripciones.append(elemento)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente extraemos los enlaces\n",
    "En este caso no nos sirve get_text(), porque nos devuelve el texto alque está asociado el enlace, no el enlace propiamente tal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Más info'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"a\").get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener el enlace usamos get() e indicamos que queremos el \"href\". Esto es igual para todos los sitios web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.crummy.com/software/BeautifulSoup/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"a\").get(\"href\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora repetimos el proceso: guardamos todo en una lista y luego iteramos para guardar solo los enlaces en una lista nueva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elementos_a = soup.find_all(\"a\")\n",
    "len(elementos_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces = []\n",
    "for elemento in elementos_a:\n",
    "    elemento = elemento.get(\"href\")\n",
    "    enlaces.append(elemento)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo, al inicio de la próxma clase veremos como se guarda esto en un archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.crummy.com/software/BeautifulSoup/', 'https://www.selenium.dev/', 'https://scrapy.org/']\n"
     ]
    }
   ],
   "source": [
    "print(enlaces)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forma alternativa propuesta por ChatGPT a través de consulta hecha por @Ignvci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Librerias': librerias, 'Descripción':descripciones, 'Enlace':enlaces}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Librerias</th>\n",
       "      <th>Descripción</th>\n",
       "      <th>Enlace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BeautifulSoup</td>\n",
       "      <td>Es fácil de instalar y práctica para personas ...</td>\n",
       "      <td>https://www.crummy.com/software/BeautifulSoup/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selenium</td>\n",
       "      <td>Es una herramienta diseñada originalmente para...</td>\n",
       "      <td>https://www.selenium.dev/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scrapy</td>\n",
       "      <td>Es un framework creado para hacer web scraping...</td>\n",
       "      <td>https://scrapy.org/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Librerias                                        Descripción  \\\n",
       "0  BeautifulSoup  Es fácil de instalar y práctica para personas ...   \n",
       "1       Selenium  Es una herramienta diseñada originalmente para...   \n",
       "2         Scrapy  Es un framework creado para hacer web scraping...   \n",
       "\n",
       "                                           Enlace  \n",
       "0  https://www.crummy.com/software/BeautifulSoup/  \n",
       "1                       https://www.selenium.dev/  \n",
       "2                             https://scrapy.org/  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
